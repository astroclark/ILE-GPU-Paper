
\documentclass[onecolumn]{revtex4}
\usepackage{hyperref}
\usepackage{color}
\usepackage{xspace}
\usepackage{bm}
\newcommand\editremark[1]{{\color{red} #1}}

\newcommand{\IMRPD}{\textsc{IMRPhenomD}\xspace}
\newcommand{\IMRPDT}{\textsc{IMRPhenomD\_NRTidal}\xspace}
\newcommand{\IMRP}{\textsc{IMRPhenomPv2}\xspace}
\newcommand{\SEOBP}{\textsc{SEOBNRv3}\xspace}
\newcommand{\SEOBA}{\textsc{SEOBNRv4}\xspace}
\newcommand{\SEOBAROM}{\textsc{SEOBNRv4\_ROM}\xspace}
\newcommand{\NRSur}{NRSur7dq2\xspace}
\newcommand{\TEOB}{SEOBNRv4T\xspace}
\newcommand{\Resum}{TEOBResumS\xspace}
\newcommand\RIFT{RIFT}

\begin{document}


We appreciate the referee's thoughtful suggestions, which will improve the paper.


\begin{enumerate}
\item * The introduction should be expanded to (briefly) discuss
hardware-based acceleration techniques in GW data analysis. For
example, there are significant efforts on the search side to use GPUs.
Within parameter estimation communities, much effort has gone into
optimizing LALInference including the use of multi-core environments
(using OpenMP, for example).

\noindent \textbf{Response}:  We added a sentence about pycbc, a search/parameter inference code which makes extensive
use of GPUs.    We defer a discussion of non-GPU changes to how lalinference's parallelization is implemented (e.g., MPI in the original
lalinference paper vs OpenMP) since we feel it would distract the reader to discuss the details how another code harnesses more cores
simultaneously, particularly since we are not experts in those details (which seem not to be published yet?) and since doing so doesn't reduce its overall comptuational cost.

\item * Due to the extra $<d | d >$ normalization factor in Eq 1, Eqs 1 and 7
look inconsistent to me.

\noindent \textbf{Response}:  Eq. (1) and (7) from Pankow et al are consistent.  The extra factor of $<d|d>$ eliminates the term in the likelihood which is independent of
the proposed signal $h(\lambda,\theta)$.

\item * Please write an equation showing how Q, U, V, F, and D are related
to quantities appearing in Eq 7 and Eq 8. Based on the text alone I
wasn't able to figure out the corresponding formula. Presumably, the
authors did not include this in the current draft due to the
expression being potentially messy. If this is the case, then perhaps
at least one of these matrices could be written down explicitly from
which the others could be worked out by the reader.

\noindent \textbf{Response}: We have significantly expanded the discussion between Eq. (8) and the start of section II.B
to emphasize how $Q,U,V,F,D$ are encoded into multidimensional arrays.



\item * What is the unbolded Lambda appearing on page 2 and how is it
related to the bolded version? Please define this variable in the
paper.

\noindent \textbf{Response}: We have now consistently rendered $\lambda\rightarrow \bm{\lambda}$; we apologize for the
confusion this duplicated symbol introduced.

\item * I did not quite understand what was meant by "are arrays over
extrinsic parameters" (page 2) since it appears that these parameters
are continuous variables (and not yet sampled).

\noindent \textbf{Response}: Our response to \# 3 should clarify this discussion.  We originally muddled the distinction
between evaluating the likelihood in general and on a large array of discrete  Monte Carlo samples

\item * Just to clarify: the GPU accelerated part is the
time-marginalization and not the marginalization over all of the
extrinsic parameters (Eq 4)? If this is true, then maybe state that
explicitly because it's not obvious and, additionally, the reader will
probably not be aware of the fact that this 1-d integral (rhw time
marginalization) is what looks to be the slowest part of rapidPE.

\noindent \textbf{Response}: The GPU-accelerated parts are (a) all trigonometric operations appearing in $Y$ and $F$, (b)  matrix operations
necessary to compute $Q$, including the time-of-flight array shift; and (c) the one-dimensional time marginalization.

\item * One of the main criticisms of GPU computing is the data transfer
latencies to and from the device. There should be some discussion of
what data is being transferred back and forth, at what stage of the
algorithm does this occur, and does it need to be done repeatedly.
Also, is any special care taken to reduce to the number of data
transfers? Or perhaps the problem is so computationally heavy this
isn't really an issue?

\noindent \textbf{Response}: In our response to \# 3, we now clarify the data transferred to the board (the initial
$Q,U,V$, once,  but also the samples $\theta_\alpha$, each iteration).  

In a future version of the code, described in the second paragraph of our conclusions, we intend to perform all Monte
Carlo random number generation on the GPU.  In this configuration we will not have significant data transfer on and off
the board.

\item * Relatedly, how much memory is required to store these matrices?
Since GPUs have smaller RAM sizes, is any special care or tuning
required to get the problem to fit onto the device? Later on the
authors use the ell=2 modes as example -- since the size of the
matrices grows with the number of modes, does the code need to account
for this in any way?

\noindent \textbf{Response}: While the memory footprint of our operations loosely grows linearly with the number of
modes involved, we have noticed no significant limitations on our memory footprint.  We have colleagues who use models with modes
$l \le 8$ without reporting memory footprint problems.   

The memory footprint of the initially transferred data is dominated by the $Q_{k,lm}$ time arrays is no larger than $16 kHz
\times 0.15  \lesssim 2500$ floating point numbers per mode per detector.  Even with several hundred modes, these are
quite small.
Potentially more problematic are our intermediate data products, notably the matrix of time-shifted arrays $Q$ built
from $Q_{k,lm}(\lambda,\tau)$ .  Our code has been carefully
organized to loop over detectors so the memory footprint of $Q$ is only (modes)$\times$ (extrinsic)$\times$ (time),
which is roughly $10^4$ times the footprint of each individual $Q_{k,lm}(\lambda,\tau)$ timeseries.  So many modes can
fit well within a typical 4Gb GPU memory space, even allowing for substantial overhead.


\item * What is epsilon appearing in 2c? Is it possible to quote typical
values? Is it problem-specific?


\noindent \textbf{Response}:   The factor $\epsilon$ connecting the number of effective samples to the number of Monte
Carlo trials is indeed highly problem dependent, and reflects the convergence rate of the Monte Carlo integral.
Approximately equivalently, the stanard deviation of the Monte Carlo error estimate scales as $1/\sqrt{N_{it}}$, with a
prefactor that  depends significantly on the specific integrand.


\item * So the reader can more easily compare the performance diagnostics,
please state the most relevant hardware specs of the GPU/CPUs used in
Sec 3 (processor speed, number of cores on the GPU, and memory size)

\noindent \textbf{Response}:    \editremark{ADD ME}

\item * Sec 3b introduces many new variables, a handful of which are
undefined. Please make sure all variables have been defined. Better
still, if possible, I believe the readability of this section could be
improved if all of the variables their definitions are placed into a
table.

\noindent \textbf{Response}:

\item * In table 1: why does the setup time reduce when more modes are
included? (comparing the first two rows of this table)

\noindent \textbf{Response}:  We perform our timing estimates on individual runs on busy nodes and clusters on which we
have shared access, not
dedicated hardware.  We anticipate small timing discrepancies from run to run.

GO BACK AND CHECK THESE NUMBERS

\item * The authors focus on computing posteriors of intrinsic parameters.
But for low-latency studies, the extrinsic parameters are arguably
more important as they can be used to direct optical telescopes. Can
the authors please comment on what changes (if any) are needed to do
low-latency studies for producing, for example, sky maps? Is this a
"trivial" change to the code or will it require more significant
re-working? I ask because fully Bayesian low-latency sky maps would be
very useful, so if this code can currently do that then it would be
worth mentioning it more prominently. Otherwise, it would be worth
discussing what additional work will be needed to realize this
promise.

\noindent \textbf{Response}:  We have added a brief comment in our conclusions to emphasize that extrinsic information
has been produced with a similar pipeline to the one used here and will be made rapidly available in an automated mode very soon.

We point out however that our inference strategy does not account for the effect of calibration error, which can have a
notable impact on sky localization: it's not immediately clear our strategy would necessarily provide a dramatic change
over the Bayestar skymaps, except insofar as we perform inference with a fully precessing signal model (and/or a model
containing higher modes).     

\item * Fig 1's caption: what does "on the evaluation points" mean? What are
the evaluation points -- maybe this could be explained in a bit more
detail in the main body of the paper.

\noindent \textbf{Response}:

\item * Figure 1: is the quantity being plotted in the lower left corner
actually the mean of $-log(L_{marg})$? (not the minus sign).

\noindent \textbf{Response}:

\item * Figure 2: same questions I have for figure 1 apply here.

\noindent \textbf{Response}:

\item * Page 7: "Our initial grid consists of 5000 points, spread uniformly
across a 4-dimensional hypercube"... do the authors mean to say $9^4 =
6561$ points?

\noindent \textbf{Response}:

\item * To give the timing numbers (ie the walltime) a bit more context,
please consider adding the time it would take to do a similar analysis
(same recovery model, set of modes, and similar CPU hardware) with
LALInference. Even rough estimates would be very helpful towards
understanding the full impact of the GPU-accelerated rapidPE code. I
feel like this is one of the most important points, as it is the key
to the impact of this work.

\noindent \textbf{Response}:


\item * The fonts in some of the figures, and especially figure 3, are small
and in almost unreadable. Please improve the readability of these
figures.

\noindent \textbf{Response}:


\item * I would appreciate it if the authors could explain to me why the
evidence converges much more quickly in figure 3 as compared to
figures 1 and 2. Or said in a different way, way are the shapes of
evidence vs iteration so different. They may optionally wish to
comment on this in the paper too.

\noindent \textbf{Response}:

\end{enumerate}

We also fixed the following proofreading issues identified by the referee
\begin{itemize}


\item * "Reducing the sampling rate by a factor s will the cost of all
operations with timeseries – they are shorter."

\noindent \textbf{Response}:


\item * "or by marginally more conservative convergence thresholds. on N
eval or N it"

\noindent \textbf{Response}:

\item *"Fortunately, the number of observationally significant and
accessible dimensions is often substantially less than the a prior
necessary dimensionality."

\noindent \textbf{Response}:

\end{itemize}
\end{document}
